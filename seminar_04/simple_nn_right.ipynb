{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Functions \n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>\n",
    "<img src=\"img/bp.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"./img/loss.png\" width=\"300\">\n",
    "<img src=\"./img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    exp_scores = np.exp(x) \n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "    loss = np.log(probs[range(N),y])\n",
    "    loss = -float(np.sum(loss)) / len(y)\n",
    "    dx = probs\n",
    "    dx[range(N),y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      "1.01391771191\n"
     ]
    }
   ],
   "source": [
    "print 'loss is a scalar\\n', loss(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      "[-0.07090429  0.03682753  0.03407676  0.0434172   0.02683517 -0.07025237\n",
      " -0.06966497  0.02354675  0.04611822  0.02788587 -0.0683026   0.04041674\n",
      " -0.07864474  0.03826714  0.04037759 -0.07687782  0.04187606  0.03500177\n",
      "  0.03813879 -0.06248451  0.02434572  0.03951789 -0.07920243  0.03968453\n",
      "  0.04560152  0.03115246 -0.07675398 -0.06944312  0.02834623  0.04109688]\n"
     ]
    }
   ],
   "source": [
    "print 'gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 8.38181293646e-08\n"
     ]
    }
   ],
   "source": [
    "print 'difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    out = np.dot(x.reshape(x.shape[0],np.prod(x.shape[1:])), w) + b\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print 'Testing affine_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    dx = dout.dot(w.T)\n",
    "    dw = (x.reshape(x.shape[0],np.prod(x.shape[1:]))).T.dot(dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  4.31043960439e-10\n",
      "dw error:  1.80120308231e-10\n",
      "db error:  2.5896892292e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "dx_num = dx_num.reshape(dx_num.shape[0], -1)\n",
    "# The error should be around 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    out = np.maximum(0, x) \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print 'Testing relu_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    dx = dout\n",
    "    dx[x <= 0]  = 0\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27560561328e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taraskhakhulin/Library/Python/2.7/lib/python/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: \t tr_loss 2.35 \t te_loss 1.92 \t te_acc 0.0925925925926\n",
      "epoch 1000: \t tr_loss 2.32 \t te_loss 2.03 \t te_acc 0.0925925925926\n",
      "epoch 2000: \t tr_loss 2.29 \t te_loss 2.13 \t te_acc 0.0925925925926\n",
      "epoch 3000: \t tr_loss 2.29 \t te_loss 2.20 \t te_acc 0.0925925925926\n",
      "epoch 4000: \t tr_loss 2.28 \t te_loss 2.22 \t te_acc 0.188888888889\n",
      "epoch 5000: \t tr_loss 2.27 \t te_loss 2.17 \t te_acc 0.161111111111\n",
      "epoch 6000: \t tr_loss 2.25 \t te_loss 2.12 \t te_acc 0.153703703704\n",
      "epoch 7000: \t tr_loss 2.23 \t te_loss 2.06 \t te_acc 0.162962962963\n",
      "epoch 8000: \t tr_loss 2.19 \t te_loss 1.99 \t te_acc 0.175925925926\n",
      "epoch 9000: \t tr_loss 2.11 \t te_loss 1.91 \t te_acc 0.185185185185\n",
      "epoch 10000: \t tr_loss 2.09 \t te_loss 1.82 \t te_acc 0.181481481481\n",
      "epoch 11000: \t tr_loss 2.05 \t te_loss 1.76 \t te_acc 0.185185185185\n",
      "epoch 12000: \t tr_loss 2.01 \t te_loss 1.72 \t te_acc 0.185185185185\n",
      "epoch 13000: \t tr_loss 2.07 \t te_loss 1.70 \t te_acc 0.188888888889\n",
      "epoch 14000: \t tr_loss 2.03 \t te_loss 1.68 \t te_acc 0.192592592593\n",
      "epoch 15000: \t tr_loss 2.06 \t te_loss 1.66 \t te_acc 0.194444444444\n",
      "epoch 16000: \t tr_loss 2.02 \t te_loss 1.66 \t te_acc 0.196296296296\n",
      "epoch 17000: \t tr_loss 2.04 \t te_loss 1.65 \t te_acc 0.198148148148\n",
      "epoch 18000: \t tr_loss 1.94 \t te_loss 1.65 \t te_acc 0.211111111111\n",
      "epoch 19000: \t tr_loss 1.97 \t te_loss 1.64 \t te_acc 0.224074074074\n",
      "epoch 20000: \t tr_loss 1.97 \t te_loss 1.64 \t te_acc 0.242592592593\n",
      "epoch 21000: \t tr_loss 1.90 \t te_loss 1.62 \t te_acc 0.268518518519\n",
      "epoch 22000: \t tr_loss 1.79 \t te_loss 1.60 \t te_acc 0.301851851852\n",
      "epoch 23000: \t tr_loss 1.83 \t te_loss 1.56 \t te_acc 0.32962962963\n",
      "epoch 24000: \t tr_loss 1.77 \t te_loss 1.51 \t te_acc 0.387037037037\n",
      "epoch 25000: \t tr_loss 1.56 \t te_loss 1.44 \t te_acc 0.462962962963\n",
      "epoch 26000: \t tr_loss 1.67 \t te_loss 1.35 \t te_acc 0.498148148148\n",
      "epoch 27000: \t tr_loss 1.37 \t te_loss 1.26 \t te_acc 0.594444444444\n",
      "epoch 28000: \t tr_loss 1.26 \t te_loss 1.18 \t te_acc 0.653703703704\n",
      "epoch 29000: \t tr_loss 1.29 \t te_loss 1.08 \t te_acc 0.703703703704\n",
      "epoch 30000: \t tr_loss 1.15 \t te_loss 1.00 \t te_acc 0.757407407407\n",
      "epoch 31000: \t tr_loss 1.08 \t te_loss 0.90 \t te_acc 0.783333333333\n",
      "epoch 32000: \t tr_loss 1.01 \t te_loss 0.82 \t te_acc 0.807407407407\n",
      "epoch 33000: \t tr_loss 0.94 \t te_loss 0.74 \t te_acc 0.827777777778\n",
      "epoch 34000: \t tr_loss 0.80 \t te_loss 0.67 \t te_acc 0.835185185185\n",
      "epoch 35000: \t tr_loss 0.71 \t te_loss 0.61 \t te_acc 0.855555555556\n",
      "epoch 36000: \t tr_loss 0.73 \t te_loss 0.55 \t te_acc 0.861111111111\n",
      "epoch 37000: \t tr_loss 0.63 \t te_loss 0.50 \t te_acc 0.866666666667\n",
      "epoch 38000: \t tr_loss 0.55 \t te_loss 0.46 \t te_acc 0.875925925926\n",
      "epoch 39000: \t tr_loss 0.55 \t te_loss 0.43 \t te_acc 0.883333333333\n",
      "epoch 40000: \t tr_loss 0.49 \t te_loss 0.40 \t te_acc 0.894444444444\n",
      "epoch 41000: \t tr_loss 0.45 \t te_loss 0.37 \t te_acc 0.903703703704\n",
      "epoch 42000: \t tr_loss 0.51 \t te_loss 0.34 \t te_acc 0.912962962963\n",
      "epoch 43000: \t tr_loss 0.45 \t te_loss 0.32 \t te_acc 0.916666666667\n",
      "epoch 44000: \t tr_loss 0.43 \t te_loss 0.30 \t te_acc 0.92037037037\n",
      "epoch 45000: \t tr_loss 0.33 \t te_loss 0.29 \t te_acc 0.924074074074\n",
      "epoch 46000: \t tr_loss 0.46 \t te_loss 0.27 \t te_acc 0.92962962963\n",
      "epoch 47000: \t tr_loss 0.35 \t te_loss 0.26 \t te_acc 0.92962962963\n",
      "epoch 48000: \t tr_loss 0.41 \t te_loss 0.25 \t te_acc 0.933333333333\n",
      "epoch 49000: \t tr_loss 0.35 \t te_loss 0.24 \t te_acc 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "std = 1e-4\n",
    "\n",
    "W1, b1 = std*np.random.random((64, 100)), np.random.random(100)\n",
    "W2, b2 = std*np.random.random((100, 10)), np.random.random(10)\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    fc_1 = np.dot(X, W1) + b1\n",
    "    score_1 = np.maximum(0, fc_1)\n",
    "    scores = np.dot(score_1, W2) + b2\n",
    "\n",
    "    \n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2, W2, b2) # Dense Layer \n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    # Backward Pass\n",
    "    dlinear2, dw2, db2 = affine_backward(dx, cache3)\n",
    "    drelu = relu_backward(dlinear2, cache2)\n",
    "    dlinea1r, dw1, db1 = affine_backward(drelu, cache1)\n",
    "    W1 = W1 - dw1 * lr\n",
    "    b1 = b1 - lr*db1\n",
    "    b2 = b2 - lr*db2\n",
    "    W2 = W2 - lr*dw2\n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    scores = np.dot(np.maximum(0, np.dot(X_test, W1)+ b1), W2) + b2\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    te_loss,_ = softmax_loss(scores, y_pred)\n",
    "    # Predict\n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch %s:' % i, \n",
    "        print '\\t tr_loss %.2f' % tr_loss,\n",
    "        print '\\t te_loss %.2f' % te_loss,\n",
    "        print '\\t te_acc %s' % accuracy_score(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
