{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Functions \n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>\n",
    "<img src=\"img/bp.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"./img/loss.png\" width=\"300\">\n",
    "<img src=\"./img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from past.builtins import xrange \n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    d = x - np.max(x, axis=1, keepdims=True)\n",
    "    softmax = np.exp(d)/np.sum(np.exp(d), axis=1, keepdims=True)\n",
    "    loss = -np.sum(np.log(softmax[xrange(x.shape[0]), y]))/x.shape[0]\n",
    "    scores = np.copy(x)\n",
    "    scores[xrange(scores.shape[0]), y] = 1\n",
    "    scores[np.where(scores!=1)] = 0\n",
    "    #print(x)\n",
    "    #print(scores)\n",
    "    #print(x[xrange(x.shape[0]), y])\n",
    "    softmax = softmax-scores\n",
    "    #softmax = -x\n",
    "    #softmax[xrange(softmax.shape[0]),y[np.newaxis,:]] +=1\n",
    "    #pint(softmax)\n",
    "    #d[xrange(softmax.shape[0]), y] = 1\n",
    "    #d[np.where(d!=1)] = 0\n",
    "    #d -= softmax\n",
    "    #d = -d\n",
    "    return loss, softmax/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      " 1.34538942759\n"
     ]
    }
   ],
   "source": [
    "print('loss is a scalar\\n', loss(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      " [ 0.03957672 -0.07454262  0.0349659  -0.07725346  0.02959367  0.0476598\n",
      "  0.02920757  0.03434955 -0.06355712  0.04624621 -0.06819754  0.02195133\n",
      "  0.02379353 -0.06371713  0.03992359  0.04463807 -0.07200298  0.02736492\n",
      " -0.06579632  0.03876053  0.02703579  0.02197117  0.05266605 -0.07463723\n",
      " -0.06196307  0.03896391  0.02299916  0.0287646   0.02470405 -0.05346865]\n"
     ]
    }
   ],
   "source": [
    "print('gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 6.7662845062e-08\n"
     ]
    }
   ],
   "source": [
    "print('difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    out = np.dot(x.reshape(x.shape[0], -1), w) + b\n",
    "    ##########################.###################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    dx = dout.dot(w.T).reshape(x.shape[0], *x.shape[1:])\n",
    "    dw = x.reshape(x.shape[0], -1).T.dot(dout).reshape(-1, dout.shape[1])\n",
    "    db = np.sum(dout, axis=0).reshape(-1)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  2.80080621817e-10\n",
      "dw error:  2.10168894169e-10\n",
      "db error:  8.49327554214e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    out = np.maximum(0, x)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    x = np.maximum(0, x)\n",
    "    x[np.where(x!=0)] = 1\n",
    "    dx = np.multiply(dout, x)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27560387141e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = (X-np.mean(X, axis = 0))/(np.var(X, axis=0)+0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efbb9498cf8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEbAgcQIOJAYAQcSI+BAYv8Dh+rwlACLpeUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbcdcbd668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cerdio/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: \t tr_loss inf \t te_loss inf \t te_acc 0.0944444444444\n",
      "epoch 1000: \t tr_loss 6.27 \t te_loss 6.65 \t te_acc 0.633333333333\n",
      "epoch 2000: \t tr_loss 1.76 \t te_loss 3.37 \t te_acc 0.811111111111\n",
      "epoch 3000: \t tr_loss 1.86 \t te_loss 2.38 \t te_acc 0.835185185185\n",
      "epoch 4000: \t tr_loss 0.98 \t te_loss 1.82 \t te_acc 0.87037037037\n",
      "epoch 5000: \t tr_loss 1.72 \t te_loss 1.74 \t te_acc 0.868518518519\n",
      "epoch 6000: \t tr_loss 0.34 \t te_loss 1.29 \t te_acc 0.887037037037\n",
      "epoch 7000: \t tr_loss 0.88 \t te_loss 1.35 \t te_acc 0.887037037037\n",
      "epoch 8000: \t tr_loss 0.57 \t te_loss 1.30 \t te_acc 0.894444444444\n",
      "epoch 9000: \t tr_loss 0.70 \t te_loss 1.09 \t te_acc 0.9\n",
      "epoch 10000: \t tr_loss 0.34 \t te_loss 1.17 \t te_acc 0.901851851852\n",
      "epoch 11000: \t tr_loss 0.10 \t te_loss 0.97 \t te_acc 0.901851851852\n",
      "epoch 12000: \t tr_loss 0.63 \t te_loss 1.07 \t te_acc 0.9\n",
      "epoch 13000: \t tr_loss 0.67 \t te_loss 1.08 \t te_acc 0.905555555556\n",
      "epoch 14000: \t tr_loss 0.06 \t te_loss 0.98 \t te_acc 0.901851851852\n",
      "epoch 15000: \t tr_loss 0.28 \t te_loss 0.87 \t te_acc 0.911111111111\n",
      "epoch 16000: \t tr_loss 0.08 \t te_loss 0.88 \t te_acc 0.914814814815\n",
      "epoch 17000: \t tr_loss 0.67 \t te_loss 0.87 \t te_acc 0.914814814815\n",
      "epoch 18000: \t tr_loss 0.25 \t te_loss 0.82 \t te_acc 0.911111111111\n",
      "epoch 19000: \t tr_loss 0.33 \t te_loss 0.85 \t te_acc 0.918518518519\n",
      "epoch 20000: \t tr_loss 0.75 \t te_loss 0.82 \t te_acc 0.92037037037\n",
      "epoch 21000: \t tr_loss 0.08 \t te_loss 0.75 \t te_acc 0.914814814815\n",
      "epoch 22000: \t tr_loss 0.13 \t te_loss 0.74 \t te_acc 0.924074074074\n",
      "epoch 23000: \t tr_loss 0.17 \t te_loss 0.70 \t te_acc 0.924074074074\n",
      "epoch 24000: \t tr_loss 0.95 \t te_loss 0.81 \t te_acc 0.92037037037\n",
      "epoch 25000: \t tr_loss 0.01 \t te_loss 0.73 \t te_acc 0.918518518519\n",
      "epoch 26000: \t tr_loss 0.02 \t te_loss 0.82 \t te_acc 0.925925925926\n",
      "epoch 27000: \t tr_loss 0.08 \t te_loss 0.73 \t te_acc 0.918518518519\n",
      "epoch 28000: \t tr_loss 0.00 \t te_loss 0.70 \t te_acc 0.925925925926\n",
      "epoch 29000: \t tr_loss 0.39 \t te_loss 0.67 \t te_acc 0.933333333333\n",
      "epoch 30000: \t tr_loss 0.16 \t te_loss 0.73 \t te_acc 0.92037037037\n",
      "epoch 31000: \t tr_loss 0.00 \t te_loss 0.67 \t te_acc 0.92962962963\n",
      "epoch 32000: \t tr_loss 0.01 \t te_loss 0.65 \t te_acc 0.925925925926\n",
      "epoch 33000: \t tr_loss 0.01 \t te_loss 0.67 \t te_acc 0.925925925926\n",
      "epoch 34000: \t tr_loss 0.04 \t te_loss 0.63 \t te_acc 0.938888888889\n",
      "epoch 35000: \t tr_loss 0.05 \t te_loss 0.70 \t te_acc 0.922222222222\n",
      "epoch 36000: \t tr_loss 0.03 \t te_loss 0.64 \t te_acc 0.935185185185\n",
      "epoch 37000: \t tr_loss 0.23 \t te_loss 0.63 \t te_acc 0.931481481481\n",
      "epoch 38000: \t tr_loss 0.50 \t te_loss 0.64 \t te_acc 0.92962962963\n",
      "epoch 39000: \t tr_loss 0.03 \t te_loss 0.62 \t te_acc 0.935185185185\n",
      "epoch 40000: \t tr_loss 0.00 \t te_loss 0.61 \t te_acc 0.933333333333\n",
      "epoch 41000: \t tr_loss 0.08 \t te_loss 0.65 \t te_acc 0.924074074074\n",
      "epoch 42000: \t tr_loss 0.03 \t te_loss 0.59 \t te_acc 0.931481481481\n",
      "epoch 43000: \t tr_loss 0.02 \t te_loss 0.57 \t te_acc 0.933333333333\n",
      "epoch 44000: \t tr_loss 0.05 \t te_loss 0.62 \t te_acc 0.931481481481\n",
      "epoch 45000: \t tr_loss 0.03 \t te_loss 0.59 \t te_acc 0.92962962963\n",
      "epoch 46000: \t tr_loss 0.00 \t te_loss 0.57 \t te_acc 0.935185185185\n",
      "epoch 47000: \t tr_loss 0.00 \t te_loss 0.58 \t te_acc 0.92962962963\n",
      "epoch 48000: \t tr_loss 0.24 \t te_loss 0.62 \t te_acc 0.925925925926\n",
      "epoch 49000: \t tr_loss 0.01 \t te_loss 0.57 \t te_acc 0.937037037037\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.random((64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.random((100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    \n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    #print(batch_X.shape)\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    #print(out1.shape, out2.shape, out3.shape)\n",
    "    #print(softmax.shape)\n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    # Backward Pass\n",
    "    dx3, dw3, db3 = affine_backward(dx, cache3)\n",
    "    dx2 = relu_backward(dx3, cache2)\n",
    "    dx1, dw1, db1 = affine_backward(dx2, cache1)\n",
    "    \n",
    "    # Updates\n",
    "    W1 = W1 - lr*dw1\n",
    "    W2 = W2 - lr*dw3\n",
    "    b1 = b1 - lr*db1\n",
    "    b2 = b2 - lr*db3\n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    te_loss = 0\n",
    "    # Predict\n",
    "    out1, _ = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "    out2, _ = relu_forward(out1)              # ReLu Layer\n",
    "    out3, _ = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    te_loss, _ = softmax_loss(out3, y_test) \n",
    "    y_pred = np.argmax(out3, axis =1)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('epoch %s:' % i, '\\t', 'tr_loss %.2f' % tr_loss,'\\t', 'te_loss %.2f' % te_loss,'\\t','te_acc %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
